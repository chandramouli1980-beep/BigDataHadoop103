misc-assaingnment-1

1. What is Default replication factor and how will you change it at file level?
Ans: The default replication factor is three
We can change with the help of hdfs-site.xml command
--------------------------------------------------------------------------------------------------------------------
2. Why do we need replication factor > 1 in production Hadoop cluster?
Ans: If a particular block of memory is crashed, replication greater than one is helpful to retrieve the lost data
----------------------------------------------------------------------------------------------------------------------
3. How will you combine the 4 part-r files of a mapreduce job?
 Ans: hdfs dfs -getmerge -nl /path1/ /path2/ /path3/ /path4/
---------------------------------------------------------------------------------------------------------------------- 
4. What are the Compression techniques in HDFS and which is the best one and why?
Ans: Snappy because its storage space is high and CPU usage is low, other compression techniques are like lzo, gzip, bzip2..
------------------------------------------------------------------------------------------------------------------------- 
5. How will you view the compressed files via HDFS command?
Ans: Hadoop fs -text, but make sure to configured snappy codec in core-site.xml
------------------------------------------------------------------------------------------------------------------------ 
6. What is Secondary Namenode and its Functionalities? why do we need it?
Ans: It is a helper node and cant replace name node, it just copies th fsimage and so updated.
----------------------------------------------------------------------------------------------------------------------------- 
7. What is Backup node and how is it different from Secondary namenode?
Ans Backup node  is checkpoint node that check points and supports the online streaming of file system  edits
------------------------------------------------------------------------------------------------------------------------------ 
8. What is FSimage and editlogs and how they are related?
Ans: FSimage is snapshot file system of modification time and access time and permission, replication are stored, editlogs are like storing activities/transactions in hdfs.
------------------------------------------------------------------------------------------------------------------------------- 
9. what is default block size in HDFS? and why is it so large?
Ans: 128MB because no need to create many blocks and reduce the metadata generated per block.
-------------------------------------------------------------------------------------------------------------------------------- 
10. How will you copy a large file of 50GB into HDFS in parllel
Ans: distcp command
-------------------------------------------------------------------------------------------------------------------------------- 
11. what is Balancing in HDFS?
Ams: A uniformly utilising the data nodes in hdfs
--------------------------------------------------------------------------------------------------------------------------------- 
12. What is expunge in HDFS ?
Ans: Name itself indicates to expel something like trash files in HDFS 
Hadoop fs -expunge